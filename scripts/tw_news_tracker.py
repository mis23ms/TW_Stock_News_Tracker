from __future__ import annotations

import json
import os
import re
import time
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import quote_plus

import requests
import xml.etree.ElementTree as ET


# -----------------------------
# Settings
# -----------------------------
TZ_TAIPEI = timezone(timedelta(hours=8))
DAYS_LOOKBACK = int(os.getenv("DAYS_LOOKBACK", "7"))
NEWS_PER_STOCK = int(os.getenv("NEWS_PER_STOCK", "3"))

INCLUDE_KEYWORDS = ["è²¡å ±", "ç‡Ÿæ”¶", "æ³•èªªæœƒ", "EPS"]
EXCLUDE_KEYWORDS = [
    "æŠ€è¡“åˆ†æ", "Kç·š", "å‡ç·š", "ç±Œç¢¼", "ç•¶æ²–", "é£†è‚¡", "çŸ­ç·š", "æ³¢æ®µ", "å¤šç©º",
    "ç›®æ¨™åƒ¹", "æ“ä½œ", "é¸è‚¡", "ç›¤ä¸­", "æ”¶ç›¤", "æ¼²åœ", "è·Œåœ", "è²·é»", "è³£é»",
    "facebook", "FB", "YouTube", "å½±ç‰‡", "æ‡¶äººåŒ…", "ç›´æ’­",
]

GOOGLE_NEWS_RSS_BASE = "https://news.google.com/rss/search"
GOOGLE_NEWS_PARAMS = {
    "hl": "zh-TW",
    "gl": "TW",
    "ceid": "TW:zh-Hant",
}

# ä¸Šå¸‚æ¯æœˆç‡Ÿæ”¶å½™ç¸½
TWSE_MONTHLY_REVENUE_URL = "https://openapi.twse.com.tw/v1/opendata/t187ap05_L"
# ä¸Šæ«ƒæ¯æœˆç‡Ÿæ”¶å½™ç¸½
TPEX_MONTHLY_REVENUE_URL = "https://www.tpex.org.tw/openapi/v1/mopsfin_t187ap05_O"

ROOT = Path(__file__).resolve().parents[1]
CONFIG_PATH = ROOT / "config" / "tw_stocks.json"
REPORTS_DIR = ROOT / "reports"
INDEX_PATH = ROOT / "index.md"


@dataclass
class NewsItem:
    stock_code: str
    stock_name: str
    title: str
    url: str
    published: Optional[datetime]  # in TZ_TAIPEI if parsed


def _now_tpe() -> datetime:
    return datetime.now(TZ_TAIPEI)


def _get_first(row: Dict[str, str], keys: List[str]) -> str:
    """Return the first non-empty value among candidate keys."""
    for k in keys:
        v = str(row.get(k, "")).strip()
        if v:
            return v
    return ""


def _fmt_int_like(s: str) -> str:
    """Format numeric-looking strings with commas; keep original if not int."""
    try:
        if s.isdigit():
            return f"{int(s):,}"
        if re.fullmatch(r"\d+\.0+", s):
            return f"{int(float(s)):,}"
    except Exception:
        pass
    return s


def _build_google_rss_url(company_name: str) -> str:
    # q format example: å°ç©é›» (è²¡å ± OR ç‡Ÿæ”¶ OR æ³•èªªæœƒ OR EPS) when:7d
    q = f'{company_name} ({ " OR ".join(INCLUDE_KEYWORDS) }) when:{DAYS_LOOKBACK}d'
    q_encoded = quote_plus(q)
    params = "&".join([f"{k}={quote_plus(v)}" for k, v in GOOGLE_NEWS_PARAMS.items()])
    return f"{GOOGLE_NEWS_RSS_BASE}?q={q_encoded}&{params}"


def _extract_text(elem: Optional[ET.Element]) -> str:
    return (elem.text or "").strip() if elem is not None else ""


def _parse_pubdate(pubdate_str: str) -> Optional[datetime]:
    if not pubdate_str:
        return None
    try:
        dt = parsedate_to_datetime(pubdate_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(TZ_TAIPEI)
    except Exception:
        return None


def _title_passes_filters(title: str) -> bool:
    t = title.strip()
    if not t:
        return False

    # include keyword
    if not any(k in t for k in INCLUDE_KEYWORDS):
        return False

    # exclude keyword
    if any(k in t for k in EXCLUDE_KEYWORDS):
        return False

    return True


def _resolve_final_url(session: requests.Session, url: str) -> str:
    # Google News RSS item link may be a google "articles/..." redirect.
    # Try to follow redirect; if it fails, keep original.
    try:
        r = session.get(url, allow_redirects=True, timeout=12)
        r.raise_for_status()
        return r.url or url
    except Exception:
        return url


def _fetch_google_news_for_stock(
    session: requests.Session,
    stock_code: str,
    stock_name: str,
) -> List[NewsItem]:
    rss_url = _build_google_rss_url(stock_name)
    resp = session.get(rss_url, timeout=20)
    resp.raise_for_status()

    root = ET.fromstring(resp.content)

    items: List[NewsItem] = []
    cutoff = _now_tpe() - timedelta(days=DAYS_LOOKBACK)

    for item in root.findall(".//item"):
        title = _extract_text(item.find("title"))
        link = _extract_text(item.find("link"))
        pubdate = _extract_text(item.find("pubDate"))
        published = _parse_pubdate(pubdate)

        if not _title_passes_filters(title):
            continue

        # ç¢ºä¿æ¨™é¡ŒçœŸçš„æåˆ°é€™å®¶å…¬å¸ï¼ˆé¿å…æŠ“åˆ°åˆ¥å®¶å…¬å¸ï¼‰
        if (stock_name not in title) and (stock_code not in title):
            continue

        if published is not None and published < cutoff:
            continue

        final_url = _resolve_final_url(session, link) if link else ""
        if not final_url:
            continue

        items.append(
            NewsItem(
                stock_code=stock_code,
                stock_name=stock_name,
                title=title,
                url=final_url,
                published=published,
            )
        )

        if len(items) >= NEWS_PER_STOCK:
            break

    return items


def _fetch_monthly_revenue(session: requests.Session) -> Dict[str, Dict[str, str]]:
    """Fetch latest monthly revenue for listed (TWSE) + OTC (TPEX), keyed by stock code."""
    out: Dict[str, Dict[str, str]] = {}

    def _load(url: str) -> List[Dict[str, str]]:
        r = session.get(url, timeout=30)
        r.raise_for_status()
        data = r.json()
        return data if isinstance(data, list) else []

    for url in [TWSE_MONTHLY_REVENUE_URL, TPEX_MONTHLY_REVENUE_URL]:
        try:
            data = _load(url)
    except Exception as e:
        print(f"[WARN] æœˆç‡Ÿæ”¶ API å¤±æ•—: {url} â†’ {e}")
        continue

        for row in data:
            code = str(row.get("å…¬å¸ä»£è™Ÿ", "")).strip()
            if code:
                out[code] = row

    return out


def _format_revenue_summary(row: Optional[Dict[str, str]]) -> str:
    if not row:
        return "æœˆç‡Ÿæ”¶ï¼šæ‰¾ä¸åˆ°è³‡æ–™ï¼ˆOpenAPI ç„¡è©²å…¬å¸ä»£è™Ÿè³‡æ–™ï¼‰"

    month_rev = _get_first(row, ["ç•¶æœˆç‡Ÿæ”¶", "ç‡Ÿæ¥­æ”¶å…¥-ç•¶æœˆç‡Ÿæ”¶"])
    mom = _get_first(row, ["ä¸Šæœˆæ¯”è¼ƒå¢æ¸›(%)", "ç‡Ÿæ¥­æ”¶å…¥-ä¸Šæœˆæ¯”è¼ƒå¢æ¸›(%)"])
    yoy = _get_first(row, ["å»å¹´åŒæœˆå¢æ¸›(%)", "ç‡Ÿæ¥­æ”¶å…¥-å»å¹´åŒæœˆå¢æ¸›(%)"])

    cum_rev = _get_first(row, ["ç´¯è¨ˆç‡Ÿæ”¶", "ç‡Ÿæ¥­æ”¶å…¥-ç´¯è¨ˆç‡Ÿæ”¶"])
    cum_yoy = _get_first(row, ["å‰æœŸæ¯”è¼ƒå¢æ¸›(%)", "ç‡Ÿæ¥­æ”¶å…¥-å‰æœŸæ¯”è¼ƒå¢æ¸›(%)"])

    parts = []
    if month_rev:
        parts.append(f"å–®æœˆ {_fmt_int_like(month_rev)}")
    if mom:
        parts.append(f"MoM {mom}%")
    if yoy:
        parts.append(f"YoY {yoy}%")

    parts2 = []
    if cum_rev:
        parts2.append(f"ç´¯è¨ˆ {_fmt_int_like(cum_rev)}")
    if cum_yoy:
        parts2.append(f"ç´¯è¨ˆYoY {cum_yoy}%")

    s1 = " / ".join(parts) if parts else "å–®æœˆï¼ˆç„¡æ•¸å€¼ï¼‰"
    s2 = " / ".join(parts2) if parts2 else "ç´¯è¨ˆï¼ˆç„¡æ•¸å€¼ï¼‰"
    return f"æœˆç‡Ÿæ”¶ï¼š{s1}ï¼›{s2}"


def _load_stocks() -> List[Dict[str, str]]:
    data = json.loads(CONFIG_PATH.read_text(encoding="utf-8"))
    if not isinstance(data, list):
        raise ValueError("config/tw_stocks.json must be a list")
    return data


def _render_report(
    date_str: str,
    all_news: List[NewsItem],
    revenue_map: Dict[str, Dict[str, str]],
) -> Tuple[str, List[str]]:
    url_list: List[str] = []
    for n in all_news:
        url_list.append(n.url)

    # URL å»é‡ï¼ˆä¿æŒé †åºï¼‰
    seen = set()
    url_list = [u for u in url_list if not (u in seen or seen.add(u))]

    # group news by stock
    by_stock: Dict[Tuple[str, str], List[NewsItem]] = {}
    for n in all_news:
        by_stock.setdefault((n.stock_code, n.stock_name), []).append(n)

    lines: List[str] = []
    lines.append(f"# å°è‚¡è¿½è¹¤ â€” {date_str}")
    lines.append("")
    lines.append("## ğŸ“‹ Copy URLs for NotebookLM")
    lines.append("")
    for u in url_list:
        lines.append(u)  # URL text only

    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("## ğŸ“Š è©³ç´°å ±å‘Š")
    lines.append("")

    for (code, name), items in by_stock.items():
        lines.append(f"### {code} {name}")
        lines.append(f"- ğŸ“ˆ {_format_revenue_summary(revenue_map.get(code))}")
        for it in items:
            lines.append(f"- ğŸ“° [{it.title}]({it.url})")
        lines.append("")

    return "\n".join(lines).strip() + "\n", url_list


def _write_report(md_text: str, date_str: str) -> Path:
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    out_path = REPORTS_DIR / f"{date_str}.md"
    out_path.write_text(md_text, encoding="utf-8")
    return out_path


def _write_index(latest_report_path: Path, date_str: str) -> None:
    # simple index that links to latest report for GitHub Pages
    rel = latest_report_path.name
    content = f"# TW_Stock_News_Tracker\n\n- æœ€æ–°å ±å‘Šï¼š[{date_str}](reports/{rel})\n"
    INDEX_PATH.write_text(content, encoding="utf-8")


def main() -> None:
    stocks = _load_stocks()
    date_str = _now_tpe().strftime("%Y-%m-%d")

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.3",
    })

    # å…ˆæŠ“æœˆç‡Ÿæ”¶ï¼ˆä¸Šå¸‚+ä¸Šæ«ƒï¼‰
    revenue_map = _fetch_monthly_revenue(session)

    all_news: List[NewsItem] = []
    for s in stocks:
        code = str(s.get("è­‰åˆ¸ä»£è™Ÿ", "")).strip()
        name = str(s.get("è­‰åˆ¸åç¨±", "")).strip()
        if not code or not name:
            continue

        try:
            items = _fetch_google_news_for_stock(session, code, name)
        except Exception:
            items = []

        all_news.extend(items)
        time.sleep(0.8)  # avoid rate limit

    md_text, _ = _render_report(date_str, all_news, revenue_map)
    report_path = _write_report(md_text, date_str)
    _write_index(report_path, date_str)


if __name__ == "__main__":
    main()
